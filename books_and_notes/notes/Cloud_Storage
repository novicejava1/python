======================================================================================
Cloud Storage
======================================================================================

- Cloud Storage is a service for storing the objects in Google cloud
- Object is a immutable piece of data consisting of file of any format
- Objects are stored in buckets. Buckets are assoicated with projects. Projects can be grouped under organization
- Tools used to interact with Cloud Storage
    Google Console
    gsutil (gcloud include gsutil)
    Client libraries for managing storage from Applications
    REST API's (ie. Managing data using JSON and XML API)
- Cloud Storage uses server-side encryption to encrypt data by default
- Object retention policy can be enabled and Object versioning
- Cloud Storage is object storage database for low latency and high durability
- Data can be configured with Object Lifecycle management OLM for transition to lower cost storage classes
- Provide multiple redundancy options
- Storage classes
    Standard Storage - Accessed frequently. Good for streaming and mobile applications
    Nearline Storage - Low cost. Good for data storage upto 30 days, eg backups and multimedia content
    Coldline Storage - Very Low cost. Good for data storage upto 90 days. eg disaster recovery
    Archive Storage - Lowest Cost, Good for data that can be stored for 365 days, regulatory archives
- Buckets are the basic containers that hold data in cloud storage
- Bucket names should be unique as they reside in single Cloud Storage namespace
- Bucket names can be used in DNS record as part of a CNAME or A redirect
- Objects stored in a multi region or dual region are geo redundant
- Storage locations should be selected to balance latency, availability and bandwidht costs for data consumers

- Storage class of existing object can be changed by rewriting the object or using Object lifecycle management
- Buckets with domain names can be created by Domain Owners
- Already created buckets cannot be renamed. New buckets needs to be created and backup and restore data into it
- Bucket object storage class can be changed. For already existing objects this change will not effect
- Project owner is billed for the resource usage. But if the requestor provides a billing project with their request, the requestor project is billed
- Requester pays should be enabled at the bucket level for requester to be charged for the data retrieval
- Bucket with requester pays enabled cannot import and export from Cloud SQL

Enable Requester Pays - gsutil requesterpays set on gs://BUCKET_NAME
Disable Requester Pays - gsutil requesterpays set off gs://BUCKET_NAME
Check Requester Pay status - gsutil requesterpays get gs://BUCKET_NAME
Requester Pay request - gsutil -u PROJECT_ID cp gs://BUCKET_NAME/OBJECT_NAME OBJECT_DESTINATION
Delete a bucket - gsutil rm -r gs://BUCKET_NAME

- We can upload a max MIME type of data upto 5 TB in size
- We can do upload request in following ways
	Simple upload		(no object metadata)
	Multipart upload	(object metadata present)
	Resumable upload	(For large files also can use streaming transfers)
	parallel composite upload - file is divided into upto 32 chunks. chunks are uploaded parallely into temp object and recreated from there
	streaming upload (useful when you dont know the final size of the object and data coming from process and input to process)
- google-crc32c or crcmod must be available to download the crc32c hash
- Standard storage class bucket should be used if parallel composite upload needs to be enabled for the buckets
- Both the JSON API and XML API support uploading object chunks in parallel and recombining them into a single object using the compose operation.
- Download can be performed in following ways
	Simple download
	Streaming download
	Sliced object download

Upload object - gsutil cp OBJECT_LOCATION gs://DESTINATION_BUCKET_NAME/
Download object - gsutil cp gs://BUCKET_NAME/OBJECT_NAME SAVE_TO_LOCATION
Listing objects - gsutil ls -r gs://BUCKET_NAME/**
Copying objects - gsutil cp gs://SOURCE_BUCKET_NAME/SOURCE_OBJECT_NAME gs://DESTINATION_BUCKET_NAME/NAME_OF_COPY
Renaning objects - gsutil mv gs://BUCKET_NAME/OLD_OBJECT_NAME gs://BUCKET_NAME/NEW_OBJECT_NAME
Moving objects - gsutil mv gs://SOURCE_BUCKET_NAME/SOURCE_OBJECT_NAME gs://DESTINATION_BUCKET_NAME/DESTINATION_OBJECT_NAME
Change storage class of objecct - gsutil rewrite -s STORAGE_CLASS gs://PATH_TO_OBJECT

- Object Versioning retains a noncurrent object version when the live object version gets replaced or deleted.
- Object versioning is enabled at the bucket level
- Noncurrent versions are uniquely identified by their generation number
- generation and metageneration are the two properties that identify the Object version

Enable Object versioning - gsutil versioning set on gs://BUCKET_NAME
Disable Object versioning - gsutil versioning set off gs://BUCKET_NAME
Check Object versioning state - gsutil versioning get gs://BUCKET_NAME
List versioned and current objects - gsutil ls -a gs://BUCKET_NAME

- Data retention policy for cloud storage bucket governs how long objects in the bucket are retained
- Detailed audit logging mode can be enabled for these immutable storage on Cloud storage. They also help with regulatory and compliance requirements
- Retention policy applies applies to existing objects in the bucket as well as new objects
- Retention policy can be locked permanently to a bucket and cannot be remove later
- Retention expiration time metadata keeps track of the object retention
- Retention policy and object versioning are mutually exclusive features in Cloud storage. Only one of them can be enabled at a time
- The maximum retention preiod of 100 years can be set

Set retention policy - gsutil retention set TIME_DURATION gs://BUCKET_NAME
Remove retention policy - gsutil retention clear gs://BUCKET_NAME
Lock retention policy - gsutil retention lock gs://BUCKET_NAME
View bucket retention - gsutil retention get gs://BUCKET_NAME

- Bucket Objects placed on hold cannot be deleted or replaced
- Object holds are metadata flags that we place on individual objects
- Cloud storage offers two types of holds
	Event based holds
	Temporary holds
When retention policy are present on the bucket
- An event based hold resets the objects time in the bucket	(loan documents)
- A temporary hold does not affect the object time in the bucket (some trade investegating docs)

Enable default event based property - gsutil retention event-default set gs://BUCKET_NAME
Get default hold status - gsutil ls -L -b gs://BUCKET_NAME
Disable default event based property - gsutil retention event-default release gs://BUCKET_NAME
Placing object hold - gsutil retention HOLD_TYPE set gs://BUCKET_NAME/OBJECT_NAME
Releasing objet hold - gsutil retention HOLD_TYPE release gs://BUCKET_NAME/OBJECT_NAME

- Object lifecycle management help in setting the below
	Time to live for objects
	retaining non current versions
	downgrading storage class for managing costs
- Object lifecycle management configuraiton can be applied to bucket. This configuration contains a set of rules which apply on bucket objects

Enable lifecycle management using json file - gsutil lifecycle set LIFECYCLE_CONFIG_FILE gs://BUCKET_NAME
Get lifecycle management - gsutil lifecycle get gs://BUCKET_NAME

- Access control for buckets can be managed using two ways
	Using IAM (Uniform access control)
	Using ACL's
- IAM controls permissioning throughout google cloud and allows to grant permissions at the bucket and project level
- IAM cannot detect permissions granted by ACL's and vice versa
- Signed URL's give time limited read/write access to an object through a URL generated
- Signed policy documents specify what can be uploaded to a bucket
- Firebase security rules provide granular attribute based access control to mobile and web apps using Firebase SDK for cloud storage

- Pub/Sub notification sends information about changes to objects in buckets to Pub/Sub topic in the form of message
- Notification can be sent to any Pub/Sub topic in any project with sufficient permissions. Pub/Sub topic can further send the message to any 
number of subscribers to the topic
- Other notification options
	Cloud Functions - Used to trigger a lightweight stand alone function in response to events and dont want to manage Pub/Sub topics. Cloud function can
execute Javascript, Python or Go function. Function and bucket should be in same project
	Object change notification - This is legacy feature of Cloud storage that sends HTTPS message to client applicaiton for Object change notification. Pub/Sub is 
cheaper and is recommended in its place.

- Notification configuration is a rule we attach to Bucket
	Topic in Pub/Sub to receive notificaiton
	Events to trigger notification
	Info contained within notifications
- Total 100 notifications can be configured per bucket and 10 notifications for each specific event
- Event types that can trigger notification are Object Create, Delete, Archive, Data or Metadata update
- Notification format sent to topic consist of two parts
	Attributes - key/value pari describing event
	Payload - A String that contains metadata of the changed object
- Cloud storage guarantees at least once delivery to Pub/Sub and also Pub/Sub guarantees at least once delivery to recipents
- Cloud storage deletes the notification after 7 days if it cannot deliver 
- Enable Pub/Sub API for the Project that will receive notification	

Create notification configuration for bucket - gsutil notification create -t TOPIC_NAME -f json gs://BUCKET_NAME
Listing notificaiton configuraiton - gsutil notification list gs://BUCKET_NAME
Delete notification configuration - gsutil notification delete projects/_/buckets/BUCKET_NAME/notificationConfigs/CONFIGURATION_NAME

- We can use Cloud Audit logs to generate logs for API operations performed in Cloud Storage
- There are two types of logs Admin Activity logs and Data Access logs
- Cloud Audit logs does not log actions taken by Object Lifecycle management

- Cloud Storage offers usage and storage logs in the form of CSV format that can be downloaded and viewed
- Usage logs provide information about all of the requests made on a specified bucket and created hourly
- Storage logs provide information about the storage consumption by bucket
- Usage logs track changes by Object lifecycle management feature
- Storage and usage logs can be analysed using Google BigQuery which enables fast SQL like queries against append only tables

- A Project organizes all Google cloud resources. It contains a set of users, a set of API's, billing, authenticaiton and monitoring settings for those API's
- For each project you use IAM to grant the ability to manage and work on your projects
- Service accounts allows applications to authenticate and access Google cloud resources and services
- Service accounts created within project have unique email address that identifies them. Services accounts can be created manually and some created automatically
by the Google Cloud

- Hash based message authentication code keys can be used to authenticate requests to Cloud Storage
- HMAC keys are type of credentials that can be associated with service accounts or user accounts in cloud storage
- HMAC key contain two piece of information
	Access ID - 24 characters alphanumeric
	Secret - 40 character base64 encoded
- HMAC keys are useful when we want to move data between other cloud storage providers and Cloud storage

Create HMAC key - gsutil hmac create SERVICE_ACCOUNT_EMAIL
Get HMAC key - gsutil hmac get KEY_ACCESS_ID 
Delete HMAC key - gsutil hmac update -s STATE ACCESS_KEY_ID
